/**
 * Copyright 2022,2023 Patrick R. Nicolas. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package org.bertspark.transformer.representation

import ai.djl.inference.Predictor
import ai.djl.ndarray._
import ai.djl.training.dataset.Batch
import java.util.ArrayList
import org.apache.spark.sql._
import org.bertspark._
import org.bertspark.classifier.training.TClassifier.predictionCount
import org.bertspark.config.MlopsConfiguration._
import org.bertspark.config.MlopsConfiguration.DebugLog._
import org.bertspark.nlp.trainingset.{implicits => _, _}
import org.bertspark.transformer.block.BERTFeaturizerBlock
import org.bertspark.transformer.config.BERTConfig
import org.bertspark.transformer.dataset._
import org.bertspark.transformer.model.TransformerModelLoader
import org.bertspark.transformer.representation.SegmentEmbeddingSimilarity.segmentEmbeddingsSimilarity
import org.bertspark.util.io._
import org.bertspark.util.TProgress
import org.slf4j._
import scala.collection.mutable.ListBuffer


/**
 * Predictor for Pre-training model. The output is used as input to classifier or decoder
 * @author Patrick Nicolas
 * @version 0.3
 */
private[bertspark] final class PretrainingInference(implicit sparkSession: SparkSession)
    extends TransformerModelLoader
        with RuntimeSystemMonitor {
  import PretrainingInference._

  override val preTrainingBlock = BERTFeaturizerBlock()

  // Set up the predictor
  private[this] val predictor: Option[Predictor[NDList, NDList]] = model.map(_.newPredictor())

  @inline
  final def getPredictor: Option[Predictor[NDList, NDList]] = predictor

  @inline
  final def isPredictorReady: Boolean = predictor.isDefined


  /**
   * Retrieve the keyed CLS prediction from Bert pre-training model
   * @param ndManager Reference to the current ND manager
   * @param singleS3Dataset S3 storage descriptor containing the input request used in the prediction
   * @param enc Implicit encoder for the Prediction request
   * @return List of Keyed CLS prediction
   */
  def predict(
    ndManager: NDManager,
    singleS3Dataset: SingleS3Dataset[ContextualDocument]
  )(implicit enc: Encoder[ContextualDocument]): List[KeyedValues] = predict(ndManager, singleS3Dataset.inputDataset)


  /**
   * Retrieve the keyed CLS prediction from Bert pre-training model
   * @param ndManager Reference to the current ND manager
   * @param contextualDocumentDS Dataset of contextual document
   * @return List of Keyed document embeddings {doc_id, embedding as NDArray)
   */
  def predict(
    ndManager: NDManager,
    contextualDocumentDS: Dataset[ContextualDocument]
  ): List[KeyedValues] = {
    val subNDManager = ndManager.newSubManager()
    val bertFeaturesDataset: FeaturesDataset[ContextualDocument] = buildLabeledPretraining(contextualDocumentDS)
    logDebug(logger, msg = s"buildLabeledPreTraining completed for ${contextualDocumentDS.count} contextual docs")

    val ctxDocumentDS = extractDocEmbedding(subNDManager, bertFeaturesDataset)
    subNDManager.close()
    ctxDocumentDS
  }


  /**
   * Generate the embedding vector for the [CLS] element generated by the Bert transformer.
   * {{{
   *   1. Get absolute model path
   *   2. Build the criteria associated with this model and block
   *   3. Load embedding from storage (S2, file system....)
   *   4. Initialize the predictor
   *   5. Execute prediction through the iterator
   * }}}
   * @param ndManager Reference to the local ND manager
   * @param inputDataset Training data set associated with pre-training
   * @return Batch of pair (Document id, CLS embeddings)
   * @todo Use translator from LabeledTrainData to NDList
   */
  def extractDocEmbedding(
    ndManager: NDManager,
    inputDataset: DjlDataset): List[KeyedValues] =
    if (predictor.isDefined) {
      inputDataset.prepare()

      // Retrieve the batches iterator after implicit conversion
      val batchIterator =
        try {
          val batches: java.lang.Iterable[Batch] = inputDataset.getData(ndManager)
          batches.iterator()
        } catch {
        case e: IllegalArgumentException =>
          logger.error(s"extractDocEmbedding: ${e.getMessage}")
          val list: java.util.List[Batch] = new ArrayList[Batch]()
          list.iterator()
        }

      val ndKeyedPredictions = ListBuffer[KeyedValues]()
      // Walk through the batch
      while (batchIterator.hasNext) {
        val metricsSummary = allMetrics("Pretrain predictor")
        if(metricsSummary.nonEmpty)
          logDebug(logger, msg = s"${predictionCount.get()}: $metricsSummary")

        val nextBatch: Batch = batchIterator.next()
        val docIdEmbeddings = extractClsEmbeddingBatch(nextBatch)
        if(docIdEmbeddings.nonEmpty)
          ndKeyedPredictions.appendAll(docIdEmbeddings)
        else
          logger.warn("Batch of segment embeddings is empty!")
        // Release all resources associated with this batch
        nextBatch.close()
      }
      ndKeyedPredictions.toList
    }
    else
    {
      logger.error("Predictor is undefined")
      List.empty[KeyedValues]
    }


  /**
   * Close the prediction model and underlying embedding
   */
  def close(): Unit = {
    logDebug(logger, "Pre training batch ND manager to close")
    predictor.foreach(_.close())
    model.foreach(_.close())
  }


      // -------------------------  Supporting methods --------------------

  @throws(clazz = classOf[RunOutGPUMemoryException])
  private def extractClsEmbeddingBatch(nextBatch: Batch): Seq[KeyedValues] = try {
    val nextData = nextBatch.getData

    // Apply the pre-training model
    val segmentEmbeddingBatch: NDList = predictor.get.predict(nextData)
    if(segmentEmbeddingBatch.isEmpty) {
      logDebug(logger, msg = "extractClsEmbeddingBatch batch is empty")
      close()
      Seq.empty[KeyedValues]
      throw new RunOutGPUMemoryException("Out of memory")
    }// retrieve the head NDArray

    else {
      // Attach to the Manager of next batch
      nextBatch.getManager.attachAll(segmentEmbeddingBatch)
      val ndSegmentEmbeddingBatch = segmentEmbeddingBatch.get(0)
      // Extract the batch size
      val batchSize = ndSegmentEmbeddingBatch.getShape.getShape.head.toInt

      // Extract the embeddings of document as pair {doc_id, aggregate_segment_embeddings}
      val keyedDocsEmbeddings =
        if(batchSize > 1)
          (0 until batchSize).map(processRecord(ndSegmentEmbeddingBatch, nextBatch, _)).filter(_._1.nonEmpty)
        else
          Seq[KeyedValues](processRecord(ndSegmentEmbeddingBatch, nextBatch, 0))
      keyedDocsEmbeddings
    }
  }
  catch {
    case e: ai.djl.translate.TranslateException =>
      logger.error(s"extractClsEmbeddingBatch: ${e.getMessage}")
      Seq.empty[KeyedValues]

    case e: IllegalStateException =>
      logger.error(s"extractClsEmbeddingBatch: ${e.getMessage}")
      Seq.empty[KeyedValues]

    case e: Exception =>
      logger.error(s"extractClsEmbeddingBatch: ${e.getMessage}")
      Seq.empty[KeyedValues]
  }

  private def processRecord(ndSegmentEmbeddingBatch: NDArray, nextBatch: Batch, index: Int): (String, Array[Float]) = {
    val ndEmbeddingValue = ndSegmentEmbeddingBatch.get(index)
    if (ndEmbeddingValue.isEmpty) {
      logger.warn(s"Batch prediction has no values")
      ("", null)
    }
    else {
      import org.bertspark.util.NDUtil.FloatNDArray._
      val docId = nextBatch.getIndices.get(index).asInstanceOf[String]
      val embeddingValues: Array[Float] = toVec(ndEmbeddingValue)
      (docId, embeddingValues)
    }
  }
}



private[bertspark] final object PretrainingInference {
  final private val logger: Logger = LoggerFactory.getLogger("PretrainingInference")

  def apply()(implicit sparkSession: SparkSession): PretrainingInference = new PretrainingInference

  /**
    * Load labeled data from a S3 storage and generate the Pre-training model and Labeled DJL dataset
    * @param contextualDocumentDS Dataset of Tokenized indexed training set TokenizedIndexedTrainingSet:
    *                             contextualDocument: ContextualDocument,
    *                             labelIndex: Int,
    *                             clsEmbedding: Array[Float]
    * @return FeaturesDataset instance
    */
  def buildLabeledPretraining(
    contextualDocumentDS: Dataset[ContextualDocument]
  ): FeaturesDataset[ContextualDocument] = {
    import implicits._
    import sparkSession.implicits._

    val isPreTrainingMode = true
    val bertDatasetConfig = TDatasetConfig(isPreTrainingMode)
    logDebug(logger, msg = s"Start loading ${contextualDocumentDS.count} contextual documents")

    val bertClassifierDataset: FeaturesDataset[ContextualDocument] = FeaturesDataset(
      contextualDocumentDS,
      bertDatasetConfig
    )
    bertClassifierDataset.prepare()
    bertClassifierDataset
  }
}